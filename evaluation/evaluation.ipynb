{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPspGRbNlcv8Rg6g5mMZ2bH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlkNpXFI9KRe",
        "outputId": "a1abe99c-5b9e-4b1a-873d-092660e85d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#google colab connection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch_pretrained_bert\n",
        "!pip install rouge_score\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "s_F7F1SFEEV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#imports \n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import ntpath\n",
        "import itertools\n",
        "import csv \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Tokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUZIiRFbEGiY",
        "outputId": "c5871ea7-6fcc-4c47-b1e6-775f0251c9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "SM6Fm_d5EHof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bertscore(summaries, references, language, avg):\n",
        "  from metrics.bert_score.scorer import BERTScorer\n",
        "  #languages:'en', 'de'\n",
        "  bert_scorer = BERTScorer(lang=language, idf=True, nthreads=4)\n",
        "  bert_scorer.compute_idf(references)\n",
        "  P, R, F1 = bert_scorer.score(summaries, references, batch_size=4)\n",
        "  if(avg):\n",
        "    return round(F1.mean().item(),5)\n",
        "  else:\n",
        "    return F1\n",
        "\n",
        "def compute_moverscore(summaries, references, language, avg): \n",
        "  #use XLM-Roberta\n",
        "  if(language == \"en\"):\n",
        "    os.environ['MOVERSCORE_MODEL'] = \"roberta-large\"\n",
        "  else: \n",
        "    os.environ['MOVERSCORE_MODEL'] = \"xlm-roberta-large\"\n",
        "  from metrics.moverscore2 import get_idf_dict, word_mover_score\n",
        "  idf_dict_hyp = get_idf_dict(summaries)\n",
        "  idf_dict_ref = get_idf_dict(references)\n",
        "  scores = word_mover_score(references, summaries, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords=True)\n",
        "  if(avg):\n",
        "    return round(np.mean(scores),5)\n",
        "  else:\n",
        "    return scores\n",
        "\n",
        "def compute_bartscore(summaries, references, language, avg): \n",
        "  if(language =='en'):\n",
        "    from metrics.bartscore import BARTScorer\n",
        "    model = 'facebook/bart-large-cnn'\n",
        "    bart_scorer = BARTScorer(device, checkpoint=model, bidirection=False)\n",
        "  elif(language == 'de'):\n",
        "    from metrics.mbartscore import MBARTScorer\n",
        "    model = 'facebook/mbart-large-50-many-to-many-mmt'\n",
        "    bart_scorer = MBARTScorer(device, checkpoint= model, bidirection=False)\n",
        "  scores = bart_scorer.score(summaries, references, batch_size=4)\n",
        "  if(avg):\n",
        "    return round(np.mean(scores),4)\n",
        "  else:\n",
        "    return scores \n",
        "\n",
        "def compute_rouge1(summaries, references, avg): \n",
        "  from rouge_score import rouge_scorer\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "  scores = []\n",
        "  for (summary, reference) in zip(summaries, references):\n",
        "    scores.append(scorer.score(summary,\n",
        "                      reference)[\"rouge1\"].fmeasure)\n",
        "  if(avg):\n",
        "    return round(np.mean(scores),4)\n",
        "  else:\n",
        "    return scores \n",
        "\n",
        "def compute_rougeL(summaries, references, avg): \n",
        "  from rouge_score import rouge_scorer\n",
        "  scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "  scores = []\n",
        "  for (summary, reference) in zip(summaries, references):\n",
        "    scores.append(scorer.score(summary,\n",
        "                      reference)[\"rougeL\"].fmeasure)\n",
        "  if(avg):\n",
        "    return round(np.mean(scores),4)\n",
        "  else:\n",
        "    return scores \n",
        "\n",
        "def compute_supert(summaries, sources, avg): \n",
        "  from metrics.supert import SupertMetric\n",
        "  scorer = SupertMetric()\n",
        "  scores = scorer.evaluate_batch(hyps, sources, aggregate=False)\n",
        "  if(avg):\n",
        "    return round(np.mean([el['supert'] for el in scores]),5)\n",
        "  else:\n",
        "    return [np.mean(el['supert'],5) for el in scores] \n",
        "\n",
        "def compute_MENLI(summaries, references, language, avg): \n",
        "  from metrics.MENLI.MENLI import MENLI\n",
        "  if(language == 'en'): \n",
        "    #used model = ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\n",
        "    cross_lingual=False\n",
        "    MENLI_scorer = MENLI(direction='avg', formula='e', src=False, nli_weight=0.3, combine_with='BERTScore-F', cross_lingual = cross_lingual)\n",
        "    MENLI_scores = MENLI_scorer.score_all(srcs=[], refs=references, hyps=summaries)\n",
        "  else: \n",
        "    #used model = xlm-roberta-base\n",
        "    cross_lingual = True\n",
        "    MENLI_scorer = MENLI(direction='avg', formula='e', src=False, nli_weight=0.3, combine_with='BERTScore-F', cross_lingual = cross_lingual)\n",
        "    MENLI_scores = MENLI_scorer.score_all(srcs=references, refs=references, hyps=summaries)\n",
        "  if(avg):\n",
        "    return round(np.mean(MENLI_scores),5)\n",
        "  else: \n",
        "    return MENLI_scores"
      ],
      "metadata": {
        "id": "fvk_epibEJgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(texts, metrics, avg):\n",
        "    \"\"\"\n",
        "    evaluate create a list of metric results. \n",
        "\n",
        "    :param texts: contains the texts\\summaries (used for the calculation) and meta data. \n",
        "    :param metrics: list of metrics to be calculated. \n",
        "    :param avg: True to return the average values and False to return a list of results.  \n",
        "    :return: list of metric resuls\n",
        "    \"\"\" \n",
        "  res = texts.loc[:, [\"Phase\",\"Model-Id\", \"text-id\", \"description\"]]\n",
        "  reference_summaries = texts[\"reference_summary\"].tolist()\n",
        "  generated_summaries = texts[\"generated_summary\"].tolist()\n",
        "  source_texts = texts[\"text\"].tolist()\n",
        "  for metric in metrics: \n",
        "    if(metric == \"rouge1\"):\n",
        "      rouge1 = compute_rouge1(generated_summaries, reference_summaries, avg = avg)\n",
        "      res[\"rouge1\"] = rouge1\n",
        "    elif (metric == \"rougel\"):  \n",
        "      rougel = compute_rougeL(generated_summaries, reference_summaries, avg = avg)\n",
        "      res[\"rougel\"] = rougel\n",
        "    elif (metric == \"bertscore\"):\n",
        "      bertscore = compute_bertscore(generated_summaries, reference_summaries, language, avg = avg)\n",
        "      res[\"bertscore\"] = bertscore\n",
        "    elif (metric == \"moverscore\"):\n",
        "      moverscore = compute_moverscore(generated_summaries, reference_summaries, language, avg = avg)\n",
        "      res[\"moverscore\"] = moverscore\n",
        "    elif (metric == \"bartscore\"):\n",
        "      bartscore = compute_bartscore(generated_summaries, reference_summaries, language, avg = avg)\n",
        "      res[\"bartscore\"] = bartscore\n",
        "    elif (metric == \"menli\"):\n",
        "      menli = compute_MENLI(generated_summaries, reference_summaries, language, avg = avg)\n",
        "      res[\"menli\"] = menli\n",
        "    elif (metric == \"supert\"):\n",
        "      supert = compute_supert(generated_summaries, source_texts, avg = avg)\n",
        "      res[\"supert\"] = supert\n",
        "    else: \n",
        "      raise Exception('Unsupported metric:', metric)\n",
        "  return res"
      ],
      "metadata": {
        "id": "YYM5yXw_WjDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text paths (#example hEN-DE texts)\n",
        "phase_1_path = \"hEN-DE_Phase1_with_id.csv\"\n",
        "phase_2_path = \"hEN-DE_Phase2_with_id.csv\"\n",
        "# metric results path\n",
        "csv_metrics = \"de_metrics_res.csv\""
      ],
      "metadata": {
        "id": "awtsqRA0ELZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read texts and summaries for each step \n",
        "phase1 = pd.read_csv(phase_1_path, sep=';')\n",
        "phase1[\"Phase\"] = \"Phase-1\"\n",
        "phase2 = pd.read_csv(phase_2_path, sep=';')\n",
        "phase2[\"Phase\"] = \"Phase-2\"\n",
        "#Concatenate the texts from the two phases\n",
        "texts = pd.concat([phase1, phase2])"
      ],
      "metadata": {
        "id": "D1H5H5xuEOPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nztRd3k8EPNf",
        "outputId": "b07d07a8-1168-469a-cf3c-6baca696bf93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text-id', 'model', 'description', 'id', 'text', 'reference_summary',\n",
            "       'generated_summary', 'Model-Id', 'Phase'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#metrics = [\"rouge1\",\"rougel\",\"bertscore\",\"bartscore\",\"moverscore\",\"menli\",\"supert\"]\n",
        "metrics = [\"rouge1\",\"rougel\",\"bertscore\",\"menli\"] #used metrics for the evaluation \n",
        "language = \"de\" # text & summary language for reference-based metrics \n",
        "avg = False #caclulate the average"
      ],
      "metadata": {
        "id": "PxRGLXK-W81-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a list of metric results. \n",
        "res = evaluate(texts, metrics, avg)\n",
        "print(res)\n",
        "#Save the scoring results to a csv file. This will be used to calculate the correlation with the human ranking.\n",
        "res.to_csv(csv_metrics, index=False, sep=';')"
      ],
      "metadata": {
        "id": "anTdP68YEWJa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
