{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JckjecRYTjg1"
      },
      "outputs": [],
      "source": [
        "#imports \n",
        "import sys\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import ntpath\n",
        "import itertools\n",
        "import csv \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gdown\n",
        "\n",
        "# Tokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "from scipy.stats import spearmanr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/drive/folders/1MWvEuXcX9GTBPGgvjHcBRZu3ZnbMUC8Q\"\n",
        "gdown.download_folder(url, output='human_evaluation')"
      ],
      "metadata": {
        "id": "ByXRtM0Rsw1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "YTF8ZmMsT5FE"
      },
      "outputs": [],
      "source": [
        "def get_headers(filename):\n",
        "  res =  {}\n",
        "  with open(filename, 'r') as file:\n",
        "    reader = csv.reader(file, delimiter=';')\n",
        "    headers = next(reader)\n",
        "    for header in headers: \n",
        "      res[header] =  headers.index(header)\n",
        "    #return headers.index(header)\n",
        "  return res\n",
        "\n",
        "def Extract(lst, index):\n",
        "    return [item[index] for item in lst]\n",
        "\n",
        "def Extract_and_Round(lst, it):\n",
        "    return [math.ceil(float(item[it])) for item in lst]\n",
        "\n",
        "def get_average_list(lst):\n",
        "    return round(sum(lst) / len(lst), 3)\n",
        "\n",
        "def get_average_lists(lists):\n",
        "  average_list = []\n",
        "  for i in range(len(lists[0])):\n",
        "    sum = count = 0\n",
        "    for listt in lists:\n",
        "      listf = [float(row) for row in listt] # convert element to float\n",
        "      if listf[i] >= 1:  #only valid values are used for the average calculation \n",
        "        sum += listf[i]\n",
        "        count += 1\n",
        "    if(count > 0):\n",
        "      average_list.append(sum/count)\n",
        "    else:\n",
        "      average_list.append(0)\n",
        "  return average_list\n",
        "\n",
        "def readCSVFile(Path, encoding, withHeader = False):\n",
        "  rows = []\n",
        "  with open(Path, 'r', encoding=encoding) as file:\n",
        "    reader = csv.reader(file, delimiter=';')\n",
        "    if(not withHeader): \n",
        "      next(reader)\n",
        "    for row in reader:\n",
        "        rows.append(row)\n",
        "  return rows\n",
        "\n",
        "#CSV Datei speichern \n",
        "def saveCSVFile(filename, array):\n",
        "  with open(filename, 'w', encoding='utf-8-sig') as file: \n",
        "      write = csv.writer(file, delimiter=';') \n",
        "      write.writerows(array) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_pVNrlIjnBNZ"
      },
      "outputs": [],
      "source": [
        "def get_annotations(annotation_files, dimensions, phase = 0):\n",
        "  #check valid phase\n",
        "  valid_phases = {0,1,2}\n",
        "  if(phase not in valid_phases): \n",
        "    raise ValueError(\"invalid phase id\", phase)\n",
        "  annotations = {}\n",
        "  for annotation_file in annotation_files: \n",
        "    file_name = ntpath.basename(annotation_file).replace(\".csv\",\"\")\n",
        "\n",
        "    df =  pd.read_csv(annotation_file, sep=';')\n",
        "    if(phase > 0):\n",
        "      df = df[df['Phase'] == \"Phase-\" + str(phase)]\n",
        "    rows = df.values.tolist()\n",
        "    headers = get_headers(annotation_file)\n",
        "    annotations[file_name] = {}\n",
        "\n",
        "    for header in headers: \n",
        "      if(header.lower() in dimensions):\n",
        "        annotations[file_name][header.lower()] = Extract(rows, headers[header])  \n",
        "      #exmpl: coherence \n",
        "      #coherence = Extract(rows, 1)\n",
        "      #annotations[file_name][\"coherence\"] = coherence\n",
        "  return annotations\n",
        "\n",
        "\n",
        "def get_correlations_annotators(annotations, dimensions):\n",
        "  annotator_pairs = list(itertools.combinations(annotations.keys(), 2))\n",
        "  correlations = {}\n",
        "  for pair in annotator_pairs: \n",
        "    correlations[pair] = {}\n",
        "    for dim in dimensions: \n",
        "      annotation_p1 = annotations[pair[0]][dim]\n",
        "      annotation_p2 = annotations[pair[1]][dim]\n",
        "      valid_annotation = np.array([(x, y) for x, y in zip([float(row) for row in annotation_p1], [float(row) for row in annotation_p2]) if x > 0 and y >0]).T #convert element to float and ignore value <1 or unannotated text (=0)\n",
        "\n",
        "      if(len(valid_annotation)>1 and len(valid_annotation[0])>1 and len(valid_annotation[1])>1): #more than 2 value needed for the correlation\n",
        "        correlation, _ = spearmanr(list(valid_annotation[0]), list(valid_annotation[1]))\n",
        "        if(not math.isnan(correlation)): #NaN if all row have the exact same value (deviation = 0)\n",
        "          correlations[pair][dim] = round(abs(correlation),3) #absolute value, round \n",
        "  return correlations\n",
        "\n",
        "def get_metric_eval(path, metrics, phase = 0): \n",
        "  valid_phases = {0,1,2}\n",
        "  if(phase not in valid_phases): \n",
        "    raise ValueError(\"invalid phase id\", phase)\n",
        "  metrics_eval = {}\n",
        "  df =  pd.read_csv(path, sep=';')\n",
        "  headers = get_headers(path) \n",
        "  if(phase > 0):\n",
        "    df = df[df['Phase'] == \"Phase-\" + str(phase)]\n",
        "  rows = df.values.tolist()\n",
        "  for header in headers:\n",
        "    if(header.lower() in metrics):\n",
        "      metrics_eval[header.lower()] = [float(row) for row in Extract(rows,headers[header])] #convert rows to float\n",
        "  return metrics_eval\n",
        "\n",
        "def get_correlations_metrics(metrics, average_ranking, dimensions): \n",
        "  correlations = {}\n",
        "  for metric in metrics: \n",
        "    correlations[metric] = {}\n",
        "    for dim in dimensions: \n",
        "      correlation, _ = spearmanr(metrics[metric], average_ranking[dim])\n",
        "      correlations[metric][dim] = round(abs(correlation),3)\n",
        "  return correlations\n",
        "\n",
        "\n",
        "def get_average_ranking(annotations, dimensions):\n",
        "  average_ranking = {}\n",
        "  for dim in dimensions: \n",
        "    lists = []\n",
        "    for annotator in annotations.keys(): \n",
        "      lists.append(annotations[annotator][dim])\n",
        "    average_ranking[dim]= get_average_lists(lists)\n",
        "  return average_ranking\n",
        "\n",
        "def get_average_ranking_per_model(annotation_files, models):  \n",
        "  headers = get_headers(annotation_files[0]) #since all files have the same structure\\fields -> read the headers one time\n",
        "  average_ranking_per_model= {}\n",
        "  for model in models:\n",
        "    tmp = {} #temp dictionary for the ranking per model\n",
        "    for dim in dimensions: #initialize the temp dic\n",
        "      tmp[dim] = []\n",
        "\n",
        "    average_ranking_per_model[model] = {}\n",
        "    for annotation_file in annotation_files: \n",
        "      file_name = ntpath.basename(annotation_file).replace(\".csv\",\"\")\n",
        "      df =  pd.read_csv(annotation_file, sep=';')\n",
        "      df = df[df['Model-Id'] == model] \n",
        "      rows = df.values.tolist()\n",
        "      headers = get_headers(annotation_file)\n",
        "      for header in headers: \n",
        "        if(header.lower() in dimensions):\n",
        "          tmp[header.lower()].append(Extract(rows, headers[header]))\n",
        "    for dim in dimensions: \n",
        "      average_ranking_per_model[model][dim] = get_average_list(get_average_lists(tmp[dim]))\n",
        "  return average_ranking_per_model\n",
        "\n",
        "def get_average_metrics_ranking_per_model(path):\n",
        "  df =  pd.read_csv(path, sep=';')\n",
        "  grouped_models_avg = df.groupby('Model-Id').mean()\n",
        "  return grouped_models_avg.to_dict('index')\n",
        "   \n",
        "\n",
        "def get_avg_per_model_annotator(annotations, dimensions): \n",
        "  avg_per_model_annotator = {}\n",
        "  for annotation_file in annotation_files: \n",
        "    file_name = ntpath.basename(annotation_file).replace(\".csv\",\"\")\n",
        "    avg_per_model_annotator[file_name] = {}\n",
        "    df = pd.read_csv(annotation_file, sep=';')\n",
        "    grouped_models = df.groupby('Model-Id')\n",
        "    for group in  list(grouped_models.groups.keys()): \n",
        "      avg_per_model_annotator[file_name][group] = {}\n",
        "      for dim in dimensions:\n",
        "        goup_value = grouped_models.get_group(group)\n",
        "        avg_per_model_annotator[file_name][group][dim] = goup_value[dim][goup_value[dim]>=1].mean()\n",
        "  return avg_per_model_annotator\n",
        "\n",
        "\n",
        "def get_avg_per_model(avg_per_model_annotator, dimensions):\n",
        "  avg_per_model = {}\n",
        "  for model in models:\n",
        "    avg_per_model[model] = {}\n",
        "    for dim in dimensions: \n",
        "      tmp = []\n",
        "      for annotator in avg_per_model_annotator: \n",
        "        if(not math.isnan(avg_per_model_annotator[annotator][model][dim])):\n",
        "          tmp.append(avg_per_model_annotator[annotator][model][dim])\n",
        "      avg_per_model[model][dim] = get_average_list(tmp)\n",
        "  return avg_per_model\n",
        "\n",
        "\n",
        "def get_average_system_correlation(annotation_files,csv_metrics, dimensions, metrics):\n",
        "  average_system_correlation = {}\n",
        "  #sort the average list per model - to get the same order for both\n",
        "  average_heval_ranking_per_model = dict(sorted(get_average_ranking_per_model(annotation_files, models).items()))\n",
        "  avg_metrics_ranking_per_model = dict(sorted(get_average_metrics_ranking_per_model(csv_metrics).items()))\n",
        "  #check models for both ranking\n",
        "  assert(list(average_heval_ranking_per_model.keys()) == list(avg_metrics_ranking_per_model.keys()))\n",
        "  \n",
        "  average_heval_ranking = {}\n",
        "  for dim in dimensions: \n",
        "    average_heval_ranking[dim] = [v[dim] for k, v in average_heval_ranking_per_model.items() if dim in v]\n",
        "\n",
        "  avg_metrics_ranking = {}\n",
        "  for metric in metrics: \n",
        "    avg_metrics_ranking[metric] = [v[metric] for k, v in avg_metrics_ranking_per_model.items() if metric in v]\n",
        "\n",
        "  for metric in metrics: \n",
        "    average_system_correlation[metric] = {}\n",
        "    for dim in dimensions:\n",
        "      correlation, _ = spearmanr(avg_metrics_ranking[metric], average_heval_ranking[dim])\n",
        "      average_system_correlation[metric][dim] = round(abs(correlation),3)\n",
        "  \n",
        "  return average_system_correlation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "-SP6RZZyT83-"
      },
      "outputs": [],
      "source": [
        "#human evaluation\n",
        "csv_1_en = \"human_evaluation/hDE-EN/en_heval_1.csv\"\n",
        "csv_2_en = \"human_evaluation/hDE-EN/en_heval_2.csv\"\n",
        "csv_3_en = \"human_evaluation/hDE-EN/en_heval_3.csv\"\n",
        "csv_4_en = \"human_evaluation/hDE-EN/en_heval_4.csv\"\n",
        "csv_5_en = \"human_evaluation/hDE-EN/en_heval_5.csv\"\n",
        "#metric evaluation \n",
        "csv_metrics = \"human_evaluation/hDE-EN/en_metrics_res.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "gdAy1p7KVHj-"
      },
      "outputs": [],
      "source": [
        "annotation_files = [csv_1_en, csv_2_en, csv_3_en, csv_4_en, csv_5_en]\n",
        "dimensions =  [\"coherence\", \"consistency\", \"fluency\", \"relevance\"]\n",
        "metrics = [\"rouge1\",\"rougel\",\"bertscore\",\"bartscore\",\"moverscore\",\"menli\",\"supert\"]\n",
        "\n",
        "models = [\"1\", \"2\", \"3\", \"6\", \"7\", \"101\", \"102\", \"B1\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sERO2fUcmJo"
      },
      "outputs": [],
      "source": [
        "phase  = 0\n",
        "print(\"Phase:\", phase)\n",
        "annotations = get_annotations(annotation_files,dimensions,phase)\n",
        "#annotation correlations\n",
        "correlations = get_correlations_annotators(annotations, dimensions)\n",
        "#metric correlations\n",
        "average_heval_ranking = get_average_ranking(annotations, dimensions)\n",
        "metrics_eval = get_metric_eval(csv_metrics,metrics,phase)  \n",
        "metrics_correlations = get_correlations_metrics(metrics_eval, average_heval_ranking, dimensions)\n",
        "average_system_correlation = get_average_system_correlation(annotation_files,csv_metrics, dimensions, metrics)\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Correlation between annotators\")\n",
        "print(\"-------------------------------------\")\n",
        "for dim in dimensions: \n",
        "  values = []\n",
        "  for outer_key, outer_value in correlations.items():\n",
        "    if dim in outer_value:\n",
        "      values.append(outer_value[dim])\n",
        "  print(dim, round(sum(values)/len(values),3))\n",
        "  #average values \n",
        "print(\"-------------------------------------\")\n",
        "print(\"Average ranking (gold standard)\")\n",
        "print(\"-------------------------------------\")\n",
        "for  key in average_heval_ranking: \n",
        "  print(key, round(get_average_list(average_heval_ranking[key]),2))\n",
        "\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Average metrics ranking\")\n",
        "print(\"-------------------------------------\")\n",
        "for  key in metrics_eval: \n",
        "  print(key, get_average_list(metrics_eval[key]))\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Segment-level Correlation metrics ranking vs. human eval ranking\")\n",
        "print(\"-------------------------------------\")\n",
        "for outer_key, outer_value in metrics_correlations.items():\n",
        "  for dim in dimensions: \n",
        "    if dim in outer_value:\n",
        "        print(outer_key, dim,\":\", outer_value[dim])\n",
        "  print(\"------\")\n",
        "print(\"-------------------------------------\")\n",
        "print(\"System-level Correlation metrics ranking vs. human eval ranking\")\n",
        "print(\"-------------------------------------\")\n",
        "for outer_key, outer_value in average_system_correlation.items():\n",
        "  for dim in dimensions: \n",
        "    if dim in outer_value:\n",
        "        print(outer_key, dim,\":\", outer_value[dim])\n",
        "  print(\"------\")\n",
        "\n",
        "#assest \n",
        "for dim in dimensions: \n",
        "  for metric in metrics_eval:\n",
        "    assert len(average_heval_ranking[dim]) == len(metrics_eval[metric])\n",
        "\n",
        "annotator_pairs = list(itertools.combinations(annotations.keys(), 2))\n",
        "for pair in annotator_pairs: \n",
        "  for dim in dimensions: \n",
        "    assert len(annotations[pair[0]][dim]) ==  len(annotations[pair[1]][dim])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phase = 0 # 0 : all, 1: phase 1, 2: phase 2\n",
        "\n",
        "annotations = get_annotations(annotation_files, dimensions)\n",
        "#annotation correlations\n",
        "correlations = get_correlations_annotators(annotations, dimensions)\n",
        "#metric correlations\n",
        "average_ranking = get_average_ranking(annotations, dimensions)\n",
        "metrics_eval =    get_metric_eval(csv_metrics,metrics,phase)\n",
        "\n",
        "print(average_ranking)\n",
        "metrics_correlations = {}\n",
        "for metric in metrics_eval: \n",
        "  metrics_correlations[metric] = {}\n",
        "  for dim in dimensions: \n",
        "    correlation, _ = spearmanr(metrics_eval[metric], average_ranking[dim])\n",
        "    metrics_correlations[metric][dim] = round(abs(correlation),3)"
      ],
      "metadata": {
        "id": "0_IAr138QEdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def get_average_system_correlation(annotation_files,csv_metrics, dimensions, metrics):\n",
        "average_system_correlation = {}\n",
        "#sort the average list per model - to get the same order for both\n",
        "average_heval_ranking_per_model = dict(sorted(get_average_ranking_per_model(annotation_files, models).items()))\n",
        "avg_metrics_ranking_per_model = dict(sorted(get_average_metrics_ranking_per_model(csv_metrics).items()))\n",
        "#check models for both ranking\n",
        "assert(list(average_heval_ranking_per_model.keys()) == list(avg_metrics_ranking_per_model.keys()))\n",
        "  \n",
        "average_heval_ranking = {}\n",
        "for dim in dimensions: \n",
        "  average_heval_ranking[dim] = [v[dim] for k, v in average_heval_ranking_per_model.items() if dim in v]\n",
        "\n",
        "print(average_heval_ranking)\n",
        "avg_metrics_ranking = {}\n",
        "for metric in metrics: \n",
        "  avg_metrics_ranking[metric] = [v[metric] for k, v in avg_metrics_ranking_per_model.items() if metric in v]\n",
        "\n",
        "print(avg_metrics_ranking)\n",
        "for metric in metrics: \n",
        "  average_system_correlation[metric] = {}\n",
        "  for dim in dimensions:\n",
        "    correlation, _ = spearmanr(avg_metrics_ranking[metric], average_heval_ranking[dim])\n",
        "    average_system_correlation[metric][dim] = round(abs(correlation),3)\n"
      ],
      "metadata": {
        "id": "G_Q3Rm3LxcE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}