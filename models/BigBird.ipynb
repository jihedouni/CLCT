{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jihedouni/CLCT/blob/main/Models/LED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jUn7G9JcHfdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByuxSZCKFvOZ",
        "outputId": "05cd1569-581d-43a6-82a8-e0b85e68da72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 29 11:27:47 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    42W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#check GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6egkpisF2Cp",
        "outputId": "8ef70d95-ecce-4a23-c55e-4d1c9c5aa67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "#check RAM\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpyfzKh9F3vi",
        "outputId": "a01e76d6-8f23-43de-f15f-e360ff4550da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#google colab connection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install nltk\n",
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "0xQobB3vqlX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import"
      ],
      "metadata": {
        "id": "vXoiMSUPqwBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    BigBirdPegasusForConditionalGeneration\n",
        ")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "import gdown\n",
        "\n",
        "#CSV \n",
        "import csv \n",
        "import re\n",
        "import torch"
      ],
      "metadata": {
        "id": "0iQ7slXAqnHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset\n",
        "url = \"https://drive.google.com/drive/folders/1nkNg5LZ_KNYM9kxlwAc1_Ek9H0zTEQVc\"\n",
        "gdown.download_folder(url, output='german_dataset')\n",
        "dataset_path = \"german_dataset/\""
      ],
      "metadata": {
        "id": "nKj37ZeqNUu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "model_or_path = \"pszemraj/bigbird-pegasus-large-K-booksum\"\n",
        "path_output_directory = \"res_model/\"\n",
        "encoder_max_length = 4096\n",
        "decoder_max_length = 512\n",
        "batch_size = 1 # only for tests\n",
        "train_epochs = 1 \n",
        "learning_rate = 1e-4\n",
        "seed = 42 "
      ],
      "metadata": {
        "id": "MBVOj1mZqnwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "SyLR35MirHEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"t5\" in model_or_path:\n",
        "    prefix = \"summarize: \"\n",
        "else:\n",
        "    prefix = \"\"\n",
        "\n",
        "print(prefix)"
      ],
      "metadata": {
        "id": "uGrECwjtHLny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7X9zhHqHdTK"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "6Bo4uLnPC03i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #example texts - fold 1\n",
        " train_path = dataset_path + \"German_Train_hDe_to_En_Step_1.csv\"\n",
        " test_path = dataset_path + \"German_Val_hDe_to_En_Step_1.csv\""
      ],
      "metadata": {
        "id": "7tpxuKmIC2_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "df = pd.read_csv(train_path, sep=';')\n",
        "print(df[df[\"id\"]==15])"
      ],
      "metadata": {
        "id": "MqziWCLNC3GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "df = pd.read_csv(test_path, sep=';')\n",
        "print(df[df[\"id\"]==7])"
      ],
      "metadata": {
        "id": "Md9xm9uhC6Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WatyTGvpH5Cp"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFX6bQLdHu5k"
      },
      "outputs": [],
      "source": [
        "#https://huggingface.co/transformers/v3.0.2/notebooks.html\n",
        "#https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/summarization.ipynb#scrollTo=545PP3o8IrJV\n",
        "#https://huggingface.co/pszemraj/bigbird-pegasus-large-K-booksum\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def generate_summary(input_text, model, tokenizer):\n",
        "    max_length = 512\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=encoder_max_length,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs.input_ids.to(\"cuda\")\n",
        "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return output_str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLfzBH8bIFpt"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune_model(id, model, tokenizer, dataset): \n",
        "\n",
        "  train_dataset = dataset.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=dataset.column_names,\n",
        "  )\n",
        "\n",
        "  #training arguments\n",
        "  args = Seq2SeqTrainingArguments(\n",
        "    output_dir= path_output_directory +\"checkpoints/Step\" + str(id),\n",
        "    evaluation_strategy = \"no\",  #The evaluation strategy to adopt during training.\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size, # The batch size per GPU/TPU core/CPU for training.\n",
        "    logging_steps=100, # Number of update steps between two logs if logging_strategy=\"steps\".\n",
        "    save_steps=1000, # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3, # If a value is passed, will limit the total amount of checkpoints. \n",
        "    num_train_epochs=train_epochs,\n",
        "    fp16=True,  # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
        "  )\n",
        "\n",
        "  #training\n",
        "  data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "  trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model(path_output_directory + \"Step\" + str(id))"
      ],
      "metadata": {
        "id": "y1WQ3jCneBt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id in range(1,6):\n",
        "  #fold i training dataset\n",
        "  train_path = dataset_path + \"German_Train_hDe_to_En_Step_\"+str(id)+\".csv\" \n",
        "  dataset = load_dataset('csv', data_files=train_path, delimiter=\";\", split=\"train\")\n",
        "  #load model\n",
        "  model = BigBirdPegasusForConditionalGeneration.from_pretrained(model_or_path, use_cache=False)\n",
        "  # load tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_or_path)\n",
        "  #finetuning\n",
        "  finetune_model(id, model, tokenizer, dataset)\n",
        "  print(id, \"is finetuned.\")"
      ],
      "metadata": {
        "id": "F8Mp9cw9kBFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generation"
      ],
      "metadata": {
        "id": "5dcCs8ouDcj8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IF-8uutUtb8"
      },
      "outputs": [],
      "source": [
        "input_text = \"\" #add test text \n",
        "#example: generate Summary using fold 5\n",
        "print(\"Generate Summary (Example):\")\n",
        "print(generate_summary(input_text, model, tokenizer))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}