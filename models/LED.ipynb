{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jihedouni/CLCT/blob/main/Models/LED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByuxSZCKFvOZ",
        "outputId": "05cd1569-581d-43a6-82a8-e0b85e68da72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 29 11:27:47 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    42W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#check GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6egkpisF2Cp",
        "outputId": "8ef70d95-ecce-4a23-c55e-4d1c9c5aa67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "#check RAM\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpyfzKh9F3vi",
        "outputId": "a01e76d6-8f23-43de-f15f-e360ff4550da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#google colab connection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install nltk\n",
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "0xQobB3vqlX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import"
      ],
      "metadata": {
        "id": "vXoiMSUPqwBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    AutoModelForSeq2SeqLM,\n",
        ")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "import gdown\n",
        "\n",
        "#CSV \n",
        "import csv \n",
        "import re\n",
        "import torch"
      ],
      "metadata": {
        "id": "0iQ7slXAqnHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset\n",
        "url = \"https://drive.google.com/drive/folders/1nkNg5LZ_KNYM9kxlwAc1_Ek9H0zTEQVc\"\n",
        "gdown.download_folder(url, output='german_dataset')\n",
        "dataset_path = \"german_dataset/\""
      ],
      "metadata": {
        "id": "nKj37ZeqNUu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "model_or_path = \"allenai/led-large-16384\"\n",
        "path_output_directory = \"res_model/\"\n",
        "encoder_max_length = 8192\n",
        "decoder_max_length = 512\n",
        "batch_size = 1"
      ],
      "metadata": {
        "id": "MBVOj1mZqnwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "SyLR35MirHEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7X9zhHqHdTK"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "6Bo4uLnPC03i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #example texts - fold 1\n",
        " train_path = dataset_path + \"German_Train_hDe_to_En_Step_1.csv\"\n",
        " test_path = dataset_path + \"German_Val_hDe_to_En_Step_1.csv\""
      ],
      "metadata": {
        "id": "7tpxuKmIC2_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "df = pd.read_csv(train_path, sep=';')\n",
        "print(df[df[\"id\"]==15])"
      ],
      "metadata": {
        "id": "MqziWCLNC3GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "df = pd.read_csv(test_path, sep=';')\n",
        "print(df[df[\"id\"]==7])"
      ],
      "metadata": {
        "id": "Md9xm9uhC6Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WatyTGvpH5Cp"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFX6bQLdHu5k"
      },
      "outputs": [],
      "source": [
        "#https://huggingface.co/allenai/led-large-16384\n",
        "#https://huggingface.co/transformers/v3.0.2/notebooks.html\n",
        "def process_data_to_model_inputs(batch):\n",
        "\n",
        "    source, target = batch[\"document\"], batch[\"summary\"]\n",
        "    \n",
        "    inputs = tokenizer(\n",
        "        source, padding=\"max_length\", truncation=True, max_length=encoder_max_length\n",
        "    )\n",
        "    outputs = tokenizer(\n",
        "        target, padding=\"max_length\", truncation=True, max_length=decoder_max_length\n",
        "    )\n",
        "\n",
        "    batch[\"input_ids\"] = inputs.input_ids\n",
        "    batch[\"attention_mask\"] = inputs.attention_mask\n",
        "\n",
        "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
        "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
        "    ]\n",
        "    # since above lists are references, the following line changes the 0 index for all samples\n",
        "    batch[\"global_attention_mask\"][0][0] = 1\n",
        "    batch[\"labels\"] = outputs.input_ids\n",
        "\n",
        "    batch = {k: v for k, v in inputs.items()}\n",
        "    # Ignore padding in the loss\n",
        "    batch[\"labels\"] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
        "        for l in outputs[\"input_ids\"]\n",
        "    ]\n",
        "    return batch\n",
        "\n",
        "def generate_summary(input_text, model, tokenizer):\n",
        "    max_length = 512\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=encoder_max_length,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs.input_ids.to(\"cuda\")\n",
        "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
        "    global_attention_mask = torch.zeros_like(attention_mask)\n",
        "    global_attention_mask[:, 0] = 1\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return output_str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLfzBH8bIFpt"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune_model(id, model, tokenizer, dataset): \n",
        "\n",
        "  train_dataset = dataset.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=dataset.column_names,\n",
        "  )\n",
        "\n",
        "  train_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        "  )\n",
        "  #training without evaluation\n",
        "  training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir= path_output_directory + \"Step\" + str(id),\n",
        "    do_train=True,\n",
        "    evaluation_strategy=\"no\", #The evaluation strategy to adopt during training.\n",
        "    per_device_train_batch_size=batch_size,  # The batch size per GPU/TPU core/CPU for training.\n",
        "    fp16=True,   # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
        "    logging_steps=100, # Number of update steps between two logs if logging_strategy=\"steps\".\n",
        "    save_steps=1000, # Number of updates steps before two checkpoint saves if save_strategy=\"steps\".\n",
        "    weight_decay=0.1,\n",
        "    label_smoothing_factor=0.1,\n",
        "    logging_dir=\"logs\",\n",
        "    save_total_limit=2, # If a value is passed, will limit the total amount of checkpoints. \n",
        "    optim=\"adamw_torch\",\n",
        "    gradient_accumulation_steps=4, # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
        "  )\n",
        "  data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "  trainer = Seq2SeqTrainer(\n",
        "    model= model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator, #The function to use to form a batch from a list of elements of train_dataset or eval_dataset\n",
        "    train_dataset=train_dataset, #The dataset to use for training.\n",
        "    tokenizer=tokenizer, # The tokenizer used to preprocess the data. \n",
        "  )\n",
        "  trainer.train()\n",
        "  trainer.save_model(path_output_directory + \"Step\" + str(id))"
      ],
      "metadata": {
        "id": "y1WQ3jCneBt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id in range(1,6):\n",
        "  #fold i training dataset\n",
        "  train_path = dataset_path + \"German_Train_hDe_to_En_Step_\"+str(id)+\".csv\" \n",
        "  dataset = load_dataset('csv', data_files=train_path, delimiter=\";\", split=\"train\")\n",
        "  #load model\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(model_or_path, use_cache=False)\n",
        "  # set generate hyperparameters\n",
        "  model.config.num_beams = 4\n",
        "  model.config.max_length = 512\n",
        "  model.config.min_length = 100\n",
        "  model.config.length_penalty = 2.0\n",
        "  model.config.early_stopping = True\n",
        "  model.config.no_repeat_ngram_size = 3\n",
        "  #model     = model.to(device)\n",
        "  # load tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_or_path)\n",
        "  #finetuning\n",
        "  finetune_model(id, model, tokenizer, dataset)\n",
        "  print(id, \"is finetuned.\")"
      ],
      "metadata": {
        "id": "F8Mp9cw9kBFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generation"
      ],
      "metadata": {
        "id": "5dcCs8ouDcj8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IF-8uutUtb8"
      },
      "outputs": [],
      "source": [
        "input_text = \"\" #add test text \n",
        "#example: generate Summary using fold 5\n",
        "print(\"Generate Summary (Example):\")\n",
        "print(generate_summary(input_text, model, tokenizer))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}