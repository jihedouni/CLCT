{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#google colab connection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RuyaHwXTGqc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nianlonggu/MemSum.git\n",
        "##https://github.com/nianlonggu/MemSum"
      ],
      "metadata": {
        "id": "AWQRhh1cGsx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"MemSum\")"
      ],
      "metadata": {
        "id": "2GAk2NtkGt9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "id": "mDZKV6X4Gvf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "10D4SQSLGwon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbHj4r92FD0A"
      },
      "outputs": [],
      "source": [
        "#CSV \n",
        "import csv \n",
        "import re\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "from src.data_preprocessing.MemSum.utils import greedy_extract\n",
        "import json\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from summarizers import MemSum\n",
        "from tqdm import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "vs0A9k4tFIkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deepl\n",
        "translator = deepl.Translator(\"TODO\") #add your Deepl auth_key #https://github.com/DeepLcom/deepl-python"
      ],
      "metadata": {
        "id": "t3worB7gIzDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#two methods to read and save CSV files \n",
        "def readCSVFile(Path, encoding, incl_header = False, delimiter = \";\"):\n",
        "  rows = []\n",
        "  with open(Path, 'r', encoding=encoding) as file:\n",
        "    reader = csv.reader(file, delimiter=delimiter)\n",
        "    if(not incl_header):\n",
        "      next(reader)\n",
        "    for row in reader:\n",
        "        rows.append(row)\n",
        "  return rows\n",
        "\n",
        "def saveCSVFile(filename, array):\n",
        "  with open(filename, 'w', encoding='utf-8-sig') as file: \n",
        "      write = csv.writer(file, delimiter=';') \n",
        "      write.writerows(array) "
      ],
      "metadata": {
        "id": "smv-95alFNVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_datasets(dataset_train_path):\n",
        "  rows = readCSVFile(dataset_train_path, 'utf-8')\n",
        "  #split train - validation \n",
        "  train_rows ,val_rows = train_test_split(rows,test_size=0.1)\n",
        "\n",
        "  train_data = []\n",
        "  for row in train_rows: \n",
        "    data = {}\n",
        "    data[\"text\"] = tokenize.sent_tokenize(row[1])\n",
        "    data[\"summary\"] = tokenize.sent_tokenize(row[2])\n",
        "    train_data.append(data)\n",
        "\n",
        "  val_data = []\n",
        "  for row in val_rows:\n",
        "    data = {}\n",
        "    data[\"text\"] = tokenize.sent_tokenize(row[1])\n",
        "    data[\"summary\"] = tokenize.sent_tokenize(row[2])\n",
        "    val_data.append(data)\n",
        "\n",
        "  return train_data, val_data\n",
        "\n",
        "def preprocessing_test_datasets(dataset_test_path):\n",
        "  rows = readCSVFile(dataset_test_path, 'utf-8')\n",
        "  test_data = []\n",
        "  for row in rows: \n",
        "    data = {}\n",
        "    data[\"text\"] = tokenize.sent_tokenize(row[1])\n",
        "    data[\"summary\"] = tokenize.sent_tokenize(row[2])\n",
        "    test_data.append(data)\n",
        "  return test_data"
      ],
      "metadata": {
        "id": "IUEVYdWDFPYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_custom_labelled_json(train_data, train_val = \"train\"):\n",
        "  for data in train_data:\n",
        "      high_rouge_episodes = greedy_extract( data[\"text\"], data[\"summary\"], beamsearch_size = 2)\n",
        "      indices_list = []\n",
        "      score_list  = []\n",
        "      for indices, score in high_rouge_episodes:\n",
        "          indices_list.append( indices )\n",
        "          score_list.append(score)\n",
        "\n",
        "      data[\"indices\"] = indices_list\n",
        "      data[\"score\"] = score_list\n",
        "\n",
        "  with open(MemSum_path+\"data/custom_data/\"+train_val+\"_CUSTOM_labelled.jsonl\",\"w\") as f:\n",
        "      for data in train_data:\n",
        "          f.write(json.dumps(data) + \"\\n\")"
      ],
      "metadata": {
        "id": "dBBKD2R1FSui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_translation(source_path, res_path, column_to_translate, target_language): \n",
        "  with open(source_path,'r') as input_file:\n",
        "    with open(res_path, 'w') as output_file:\n",
        "        res = []\n",
        "        writer = csv.writer(output_file, lineterminator='\\n',  delimiter=';')\n",
        "        reader = csv.reader(input_file,  delimiter=';')\n",
        "        row = next(reader)\n",
        "        row.append('generated_summary_translated')\n",
        "        res.append(row)\n",
        "        for row in reader:\n",
        "            translated_summary = translator.translate_text(row[column_to_translate], target_lang=target_language)\n",
        "            row.append(translated_summary)\n",
        "            res.append(row)\n",
        "        writer.writerows(res)"
      ],
      "metadata": {
        "id": "jeEwbuwQIuDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(memsum_custom_data, max_sentences, text): \n",
        "  sentences = tokenize.sent_tokenize(text)\n",
        "  extracted_summary_sentences = memsum_custom_data.extract([sentences], p_stop_thres = 0.6, max_extracted_sentences_per_document = max_sentences)[0] \n",
        "  extracted_summary = \"\\n\".join(sorted(extracted_summary_sentences, key=lambda x: text.index(x)))\n",
        "  return extracted_summary\n",
        "  \n",
        "def evaluate(model, corpus, p_stop, max_extracted_sentences, rouge_cal ):\n",
        "    scores = []\n",
        "    for data in tqdm(corpus):\n",
        "        gold_summary = data[\"summary\"]\n",
        "        extracted_summary = model.extract( [data[\"text\"]], p_stop_thres = p_stop, max_extracted_sentences_per_document = max_extracted_sentences )[0]\n",
        "        \n",
        "        score = rouge_cal.score( \"\\n\".join( gold_summary ), \"\\n\".join(extracted_summary)  )\n",
        "        scores.append( [score[\"rouge1\"].fmeasure, score[\"rouge2\"].fmeasure, score[\"rougeLsum\"].fmeasure ] )\n",
        "    return np.asarray(scores).mean(axis = 0)   "
      ],
      "metadata": {
        "id": "r4KR-QHzFUdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#used to download the glove embedding (200dim) used in MemSum, with three addition token embeddings for bos eos pad, e\n",
        "#https://github.com/nianlonggu/MemSum\n",
        "!pip install gdown -q\n",
        "try:\n",
        "    os.system(\"rm -r model\")\n",
        "    os.makedirs(\"model/\")\n",
        "except:\n",
        "    pass\n",
        "!cd model/; gdown --folder https://drive.google.com/drive/folders/1lrwYrrM3h0-9fwWCOmpRkydvmF6hmvmW\n",
        "\n",
        "\n",
        "if not os.path.exists(\"model/glove\"):\n",
        "    try:\n",
        "        os.makedirs(\"model/glove\")\n",
        "        os.system(\"mv model/*.pkl model/glove/\")\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "zns735ExJ8Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## paramater \n",
        "dataset_path = \"English\" #path need to be changed\n",
        "max_doc_len = '4096'\n",
        "max_seq_len = '512'\n",
        "num_of_epochs = '10'\n",
        "save_every = '1000'\n",
        "n_device = '2'\n",
        "batch_size_per_device = '1'\n",
        "max_extracted_sentences_per_document = '12'\n",
        "moving_average_decay = '0.999'\n",
        "p_stop_thres  = '0.6'"
      ],
      "metadata": {
        "id": "qsCdoeBXF9I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training for each fold \n",
        "for id in range(1,6): \n",
        "  fold = str(id)\n",
        "  dataset_train_path = dataset_path+\"English_Ext_Train_hEn_to_De_Step_\"+fold+\".csv\"\n",
        "  train_data, val_data =  preprocessing_datasets(dataset_train_path)\n",
        "  generate_custom_labelled_json(train_data)\n",
        "  generate_custom_labelled_json(val_data, \"val\")\n",
        "  os.system(\"python \"+MemSum_path+\"src/MemSum_Full/train.py -training_corpus_file_name \"+MemSum_path+\"data/custom_data/train_CUSTOM_labelled.jsonl -validation_corpus_file_name \"+\n",
        "            MemSum_path+\"data/custom_data/val_CUSTOM_labelled.jsonl -model_folder \"+MemSum_path+\"model/MemSum_Full/custom_data/200dim/run0/\"+fold+\"/ -log_folder \"+\n",
        "            MemSum_path+\"model/log/MemSum_Full/custom_data/200dim/run0/\"+fold+\"/ -vocabulary_file_name \"+MemSum_path+\"model/glove/vocabulary_200dim.pkl -pretrained_unigram_embeddings_file_name \"\n",
        "            +MemSum_path+\"model/glove/unigram_embeddings_200dim.pkl -max_seq_len \"+max_seq_len+\" -max_doc_len \"+max_doc_len+\"  -num_of_epochs \"+num_of_epochs+\" -save_every \"+\n",
        "            save_every+\" -n_device \"+n_device+\" -batch_size_per_device \"+batch_size_per_device+\" -max_extracted_sentences_per_document \"+max_extracted_sentences_per_document+\" -moving_average_decay \"+moving_average_decay+\" -p_stop_thres \"+p_stop_thres)\n",
        "\n"
      ],
      "metadata": {
        "id": "VwhWYzaFFXAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text generation for each fold\n",
        "for id in range(1,6): \n",
        "  fold = str(id)\n",
        "  run_path = MemSum_path+'model/MemSum_Full/custom_data/200dim/run0/'+fold+'/model_batch_990.pt'\n",
        "  print(run_path)\n",
        "  res_path = '/Generated_summaries/summ_eval_step_'+str(id)+'.csv'\n",
        "  dataset_test_path = dataset_path+\"English_Ext_Val_hEn_to_De_Step_\"+fold+\".csv\"\n",
        "  data = readCSVFile(dataset_test_path, 'utf-8')\n",
        "  test_data =  preprocessing_test_datasets(dataset_test_path)\n",
        "  memsum_custom_data = MemSum(run_path, \n",
        "                              MemSum_path+\"model/glove/vocabulary_200dim.pkl\", \n",
        "                              gpu = 0 ,  max_doc_len = 512)\n",
        "  res = []\n",
        "  res.append([\"id\",\"text\", \"reference_summary_sl\", \"reference_summary_ol\", \"generated_summary\"])\n",
        "  for row in data:\n",
        "    res.append([row[0], row[1], row[2], row[3], generate_summary(memsum_custom_data, max_extracted_sentences_per_document, row[1])])\n",
        "                 \n",
        "  saveCSVFile(res_path, res)\n",
        "  rouge_cal = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)\n",
        "  print(evaluate(memsum_custom_data, test_data, 0.6, 7, rouge_cal))"
      ],
      "metadata": {
        "id": "jnz9OdHAFYhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add translation \n",
        "for i in range(1,6):\n",
        "  source_path =\"/summ_eval_step_\"+str(i)+\".csv\"\n",
        "  res_path = source_path.replace(\".csv\", \"_with_translation.csv\")\n",
        "  #print(res_path)\n",
        "  add_translation(source_path, res_path, 4, \"DE\" )"
      ],
      "metadata": {
        "id": "gCDdOwZOJNgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}