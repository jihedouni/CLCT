{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# import rouge\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import  RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from functools import lru_cache\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "class SentenceTokenizer:\n",
    "    def __init__(self ):\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    @lru_cache(100000)\n",
    "    def stem( self, w ):\n",
    "        return self.stemmer.stem(w)\n",
    "    \n",
    "    def tokenize(self, sen ):\n",
    "        sen =  [ self.stem(w) for w in self.tokenizer.tokenize( sen.lower() )   ]\n",
    "        return sen\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self ):\n",
    "        self.word_to_index = {}\n",
    "        self.vocab_size = 0\n",
    "        self.tokenizer = SentenceTokenizer()\n",
    "    \n",
    "    def sent2seq( self, sent ):\n",
    "        seq = []\n",
    "        words = self.tokenizer.tokenize( sent )\n",
    "        for w in words:\n",
    "            if w not in self.word_to_index:\n",
    "                self.word_to_index[w] = self.vocab_size\n",
    "                self.vocab_size +=1\n",
    "            seq.append( self.word_to_index[w] )\n",
    "        return seq\n",
    "\n",
    "\n",
    "def fast_rouge_score( ref, hyp, n_gram_list = [1,2], history_results = None ):\n",
    "    # ref and hyp are lists of word indices\n",
    "    ref = np.array(ref)\n",
    "    hyp = np.array(hyp)\n",
    "\n",
    "    \n",
    "    results = {}\n",
    "    for n in n_gram_list:\n",
    "        if history_results is None:\n",
    "\n",
    "            ref_ngram = np.concatenate( [ ref[offset: len(ref) -( n-offset )+1  ][:,np.newaxis] for offset in range(n)  ], axis = 1 )\n",
    "            hyp_ngram = np.concatenate( [ hyp[offset: len(hyp) -( n-offset )+1  ][:,np.newaxis] for offset in range(n)  ], axis = 1 )\n",
    "        \n",
    "            if len(ref_ngram) == 0:\n",
    "                results[ \"rouge%d\"%(n)] = { \"precision\":0.0,\"recall\":0.0,\"fmeasure\":0.0 }\n",
    "                continue\n",
    "\n",
    "            unique_ref_ngram = np.unique( ref_ngram, axis = 0 )\n",
    "            unique_ref_ngram_expanded_for_ref = unique_ref_ngram[:,np.newaxis,:].repeat( ref_ngram.shape[0], axis = 1 )\n",
    "            n_match_in_ref = np.all(unique_ref_ngram_expanded_for_ref == ref_ngram[np.newaxis,:,:], axis = 2).sum(1)\n",
    "\n",
    "            unique_ref_ngram_expanded_for_hyp = unique_ref_ngram[:,np.newaxis,:].repeat( hyp_ngram.shape[0], axis = 1 )\n",
    "            n_match_in_hyp = np.all(unique_ref_ngram_expanded_for_hyp == hyp_ngram[np.newaxis,:,:], axis = 2).sum(1)\n",
    "            \n",
    "            n_hyp = hyp_ngram.shape[0]\n",
    "        else:\n",
    "            history_results = history_results.copy()\n",
    "            ## in this case, we assume ref is the same as before, so we directly load the necessary array from the history\n",
    "            if \"ref_ngram\" not in history_results[\"rouge%d\"%(n)] or \\\n",
    "                len(history_results[\"rouge%d\"%(n)][\"ref_ngram\"]) == 0:\n",
    "                results[ \"rouge%d\"%(n)] = { \"precision\":0.0,\"recall\":0.0,\"fmeasure\":0.0 }\n",
    "                continue    \n",
    "\n",
    "            ref_ngram = history_results[\"rouge%d\"%(n)][\"ref_ngram\"]\n",
    "            hyp_ngram = np.concatenate( [ hyp[offset: len(hyp) -( n-offset )+1  ][:,np.newaxis] for offset in range(n)  ], axis = 1 )\n",
    "            \n",
    "            # if len(ref_ngram) == 0:\n",
    "            #     results[ \"rouge%d\"%(n)] = { \"precision\":0.0,\"recall\":0.0,\"fmeasure\":0.0 }\n",
    "            #     continue\n",
    "\n",
    "            unique_ref_ngram = history_results[\"rouge%d\"%(n)][\"unique_ref_ngram\"]\n",
    "            n_match_in_ref = history_results[\"rouge%d\"%(n)][\"n_match_in_ref\"]\n",
    "\n",
    "            unique_ref_ngram_expanded_for_hyp = unique_ref_ngram[:,np.newaxis,:].repeat( hyp_ngram.shape[0], axis = 1 )\n",
    "            n_match_in_hyp = np.all(unique_ref_ngram_expanded_for_hyp == hyp_ngram[np.newaxis,:,:], axis = 2).sum(1)\n",
    "            n_match_in_hyp = n_match_in_hyp + history_results[\"rouge%d\"%(n)][\"n_match_in_hyp\"]\n",
    "            n_hyp = hyp_ngram.shape[0] + history_results[\"rouge%d\"%(n)][\"n_hyp\"]\n",
    "\n",
    "\n",
    "        n_common = np.minimum(n_match_in_ref, n_match_in_hyp )\n",
    "        p = n_common.sum() / (n_hyp+1e-12)\n",
    "        r = n_common.sum() / ref_ngram.shape[0]\n",
    "        f = 2/( 1/(r+1e-12) + 1/(p+1e-12) )\n",
    "        \n",
    "        results[ \"rouge%d\"%(n)] = { \"precision\":p,\"recall\":r,\"fmeasure\":f, \n",
    "                                    \"ref_ngram\":ref_ngram,\n",
    "                                    \"unique_ref_ngram\":unique_ref_ngram,\n",
    "                                    \"n_match_in_ref\":n_match_in_ref,\n",
    "                                    \"n_match_in_hyp\":n_match_in_hyp,\n",
    "                                    \"n_hyp\":n_hyp\n",
    "                                     }\n",
    "        \n",
    "    return  results\n",
    "\n",
    "\n",
    "def get_real_rouge_score( hyps_list, ref_list, rouge_cal ):\n",
    "    score_list = []\n",
    "    for i in range(len(hyps_list)):\n",
    "        hyp = hyps_list[i]\n",
    "        ref = ref_list[i]\n",
    "        score = rouge_cal.score( ref, hyp)\n",
    "        score_list.append(  (score[\"rouge1\"].fmeasure+score[\"rouge2\"].fmeasure+score[\"rougeLsum\"].fmeasure)/3  )\n",
    "    return score_list\n",
    "\n",
    "\n",
    "def get_score( hyp, ref, n_gram_list =[1,2], history_results = None, metric = \"fmeasure\" ):\n",
    "    res = fast_rouge_score( ref, hyp, n_gram_list, history_results )\n",
    "    score = np.mean([ res[\"rouge%d\"%(n)][metric] for n in  n_gram_list ])\n",
    "    return score, res\n",
    "\n",
    "def get_items( a_list, indices ):\n",
    "    return [ a_list[i] for i in indices ]\n",
    "\n",
    "def join_items( items ):\n",
    "    res = []\n",
    "    for item in items:\n",
    "        res += item\n",
    "    return res\n",
    "\n",
    "## by default, document is a list of seqs, summary is single seq\n",
    "def greedy_extract(document, summary, extracted_indices, beamsearch_size, max_num_extracted_sentences, max_num_extractions , candidate_extractions, epsilon, n_gram_list = [1,2]):\n",
    "    if max_num_extractions is not None and len(candidate_extractions)>=max_num_extractions:\n",
    "        return\n",
    "    current_summary = join_items(get_items(document, extracted_indices ))\n",
    "    current_score, history_results =  get_score(current_summary , summary, n_gram_list = n_gram_list  )\n",
    "    \n",
    "    if len(extracted_indices) >= max_num_extracted_sentences:\n",
    "        candidate_extractions.append(  [ extracted_indices, current_score  ] )\n",
    "        return\n",
    "    \n",
    "    remaining_indices = list(set( np.arange(len(document)) ) - set(extracted_indices))\n",
    "    if len(remaining_indices) == 0:\n",
    "        if len(extracted_indices) == 0:\n",
    "            return\n",
    "        candidate_extractions.append(  [ extracted_indices, current_score  ] )\n",
    "        return\n",
    "    \n",
    "    \n",
    "    new_scores =[]\n",
    "    summary_set = set(summary)\n",
    "    for i in  range(len(document )):  # remaining_indices:\n",
    "        new_scores.append( get_score( document[i], summary, history_results = history_results,  n_gram_list = n_gram_list  )[0] )\n",
    "    new_scores = np.array( new_scores )\n",
    "\n",
    "    gain_scores = new_scores-current_score\n",
    "\n",
    "    if not np.any( gain_scores>epsilon ):\n",
    "        if len(extracted_indices) == 0:\n",
    "            return\n",
    "        candidate_extractions.append(  [ extracted_indices, current_score  ] )\n",
    "        return\n",
    "    \n",
    "    new_poses = np.argsort( -gain_scores )[:beamsearch_size]\n",
    "    for pos in new_poses:\n",
    "        if gain_scores[pos]>0:\n",
    "            idx = pos #int(remaining_indices[pos])\n",
    "            greedy_extract(document, summary, extracted_indices.copy() + [idx], beamsearch_size,max_num_extracted_sentences, max_num_extractions , candidate_extractions, epsilon,  n_gram_list = n_gram_list )    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "\n",
    "LONDON , England -LRB- Reuters -RRB- -- Harry Potter star Daniel Radcliffe gains access to a reported # 20 million -LRB- $ 41.1 million -RRB- fortune as he turns 18 on Monday , but he insists the money wo n't cast a spell on him .##SENT##Daniel Radcliffe as Harry Potter in `` Harry Potter and the Order of the Phoenix ''##SENT##To the disappointment of gossip columnists around the world , the young actor says he has no plans to fritter his cash away on fast cars , drink and celebrity parties .##SENT##`` I do n't plan to be one of those people who , as soon as they turn 18 , suddenly buy themselves a massive sports car collection or something similar , '' he told an Australian interviewer earlier this month . `` I do n't think I 'll be particularly extravagant .##SENT##`` The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs . ''##SENT##At 18 , Radcliffe will be able to gamble in a casino , buy a drink in a pub or see the horror film `` Hostel : Part II , '' currently six places below his number one movie on the UK box office chart .##SENT##Details of how he 'll mark his landmark birthday are under wraps . His agent and publicist had no comment on his plans .##SENT##`` I 'll definitely have some sort of party , '' he said in an interview . `` Hopefully none of you will be reading about it . ''##SENT##Radcliffe 's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch .##SENT##Despite his growing fame and riches , the actor says he is keeping his feet firmly on the ground .##SENT##`` People are always looking to say ` kid star goes off the rails , ' '' he told reporters last month . `` But I try very hard not to go that way because it would be too easy for them . ''##SENT##His latest outing as the boy wizard in `` Harry Potter and the Order of the Phoenix '' is breaking records on both sides of the Atlantic and he will reprise the role in the last two films . Watch I-Reporter give her review of Potter 's latest ''##SENT##There is life beyond Potter , however .##SENT##The Londoner has filmed a TV movie called `` My Boy Jack , '' about author Rudyard Kipling and his son , due for release later this year . He will also appear in `` December Boys , '' an Australian film about four boys who escape an orphanage .##SENT##Earlier this year , he made his stage debut playing a tortured teenager in Peter Shaffer 's `` Equus . ''##SENT##Meanwhile , he is braced for even closer media scrutiny now that he 's legally an adult : `` I just think I 'm going to be more sort of fair game , '' he told Reuters . E-mail to a friend .##SENT##Copyright 2007 Reuters . All rights reserved.This material may not be published , broadcast , rewritten , or redistributed .\n",
    "\n",
    "\"\"\".strip().split(\"##SENT##\")\n",
    "summ = \"\"\"\n",
    "\n",
    "Harry Potter star Daniel Radcliffe gets # 20M fortune as he turns 18 Monday .##SENT##Young actor says he has no plans to fritter his cash away .##SENT##Radcliffe 's earnings from first five Potter films have been held in trust fund .\n",
    "\n",
    "\"\"\".strip().split(\"##SENT##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 2] 0.48888888888990767\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_gain( doc,summ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"0.207792207792 0.0816326530612 0.34375 0.0487804878049 0.0 0.0 0.0 0.0 0.406779661017 0.0754716981132 0.0 0.0246913580247 0.0 0.0 0.0 0.0 0.0\t-0.00677966101695 -0.0178907721281 0.0989674654198 -0.14011299435 -0.0867796610169 -0.159356980605 -0.106779661017 -0.106779661017 -0.114096734188 -0.0383586083854 -0.148715144888 -0.156779661017 -0.0317796610169 -0.164355418593 -0.0867796610169 -0.15414808207 -0.0824553366926\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20253165 0.07843137 0.33333333 0.04761905 0.         0.\n",
      " 0.         0.         0.39344262 0.07272727 0.         0.02409639\n",
      " 0.         0.         0.         0.         0.        ]\n",
      "[-0.00128576 -0.01506424  0.1009394  -0.13176038 -0.08175431 -0.15101838\n",
      " -0.1007597  -0.1007597  -0.10772834 -0.03446826 -0.14081104 -0.1481596\n",
      " -0.02980626 -0.15581886 -0.08175431 -0.14601994 -0.07765315]\n",
      "[-0.03087362 -0.02286947 -0.11600753 -0.15065359 -0.07379455 -0.14513889\n",
      " -0.09249249 -0.09249249 -0.09950836 -0.07767394 -0.13405018 -0.14814815\n",
      " -0.02573099 -0.15042735 -0.07379455 -0.13968254 -0.06984127]\n"
     ]
    }
   ],
   "source": [
    "vocab  = Vocab()\n",
    "document_seq = [ vocab.sent2seq(sen) for sen in doc  ]\n",
    "summary_seq = [ vocab.sent2seq(sen) for sen in summ  ]\n",
    "extracted_indices = []\n",
    "candidate_extractions = []\n",
    "greedy_extract(document_seq, join_items( summary_seq ) , extracted_indices, 1, 10, 1 ,candidate_extractions, 0.001,  n_gram_list = [2])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[8, 2], 0.48888888888990767]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_cal = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.40816326530612246, recall=0.5263157894736842, fmeasure=0.45977011494252873),\n",
       " 'rouge2': Score(precision=0.2916666666666667, recall=0.3783783783783784, fmeasure=0.3294117647058824),\n",
       " 'rougeLsum': Score(precision=0.2857142857142857, recall=0.3684210526315789, fmeasure=0.32183908045977017)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_cal.score(\"\\n\".join(get_items( doc,[9,1] ))  ,  \"\\n\".join(summ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.6122448979591837, recall=0.4, fmeasure=0.48387096774193544),\n",
       " 'rouge2': Score(precision=0.3541666666666667, recall=0.22972972972972974, fmeasure=0.27868852459016397),\n",
       " 'rougeLsum': Score(precision=0.5102040816326531, recall=0.3333333333333333, fmeasure=0.4032258064516129)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_cal.score(\"\\n\".join(get_items( doc,[9,7,1] ))  ,  \"\\n\".join(summ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Leifman says about one-third of all people in Miami-Dade county jails are mentally ill . So , he says , the sheer volume is overwhelming the system , and the result is what we see on the ninth floor .',\n",
       " \"`` I am the son of the president . You need to get me out of here ! '' one man shouts at me .\",\n",
       " \"An inmate housed on the `` forgotten floor , '' where many mentally ill inmates are housed in Miami before trial .\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " get_items( doc,[7,9,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"-input_corpus_file_name\" )\n",
    "    parser.add_argument(\"-output_corpus_file_name\" )\n",
    "    parser.add_argument(\"-beamsearch_size\", type = int, default = 2)\n",
    "    parser.add_argument(\"-max_num_extracted_sentences\", type = int, default = 20 )\n",
    "    parser.add_argument(\"-max_num_extractions\", type = int, default = 15 )\n",
    "    parser.add_argument(\"-start\", type =int, default = 0 )\n",
    "    parser.add_argument(\"-size\", type =int, default = 0 )\n",
    "    parser.add_argument(\"-epsilon\", type = float, default = 0.001 )\n",
    "    parser.add_argument(\"-truncation\", type =int, default = 10000 )  ## we keep 10000 sentences at most by default\n",
    "    parser.add_argument(\"-summary_field\" )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    rouge_cal = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "\n",
    "\n",
    "    if not ( args.start == 0 and args.size == 0 ):\n",
    "        output_corpus_file_name = args.output_corpus_file_name+\"_%d\"%(args.start)\n",
    "    else:\n",
    "        output_corpus_file_name = args.output_corpus_file_name\n",
    "\n",
    "    with open( output_corpus_file_name, \"w\" ) as fw:\n",
    "        count = 0\n",
    "        with open(args.input_corpus_file_name,\"r\") as f:\n",
    "            for line in tqdm(f):\n",
    "                if count < args.start:\n",
    "                    count +=1\n",
    "                    continue\n",
    "                if args.size>0 and count >= args.start + args.size:\n",
    "                    break   \n",
    "\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "\n",
    "                    summary = sent_tokenize(\" \".join( [ data[args.summary_field][key] for key in data[args.summary_field] ] ))\n",
    "                    \n",
    "                    document = data[\"opinion\"][:args.truncation]\n",
    "                    \n",
    "                    sub_indices = np.arange( len(document) ).tolist()\n",
    "                    sub_indices = [ int(item) for item in sub_indices ]\n",
    "\n",
    "                    vocab  = Vocab()\n",
    "                    document_seq = [ vocab.sent2seq(sen) for sen in document  ]\n",
    "                    summary_seq = [ vocab.sent2seq(sen) for sen in summary  ]\n",
    "                    extracted_indices = []\n",
    "                    candidate_extractions = []\n",
    "                    greedy_extract(document_seq, join_items( summary_seq ) , extracted_indices, args.beamsearch_size, args.max_num_extracted_sentences, args.max_num_extractions ,candidate_extractions, args.epsilon)     \n",
    "\n",
    "                    candidate_extractions.sort( key = lambda x :-x[1] )\n",
    "                    existing_extraction_indices = set()\n",
    "                    cleaned_candidate_extractions = []\n",
    "                    for extraction in candidate_extractions:\n",
    "                        extraction_indice = \"-\".join( map( str, sorted(extraction[0]) ) )\n",
    "                        if not extraction_indice in existing_extraction_indices:\n",
    "                            existing_extraction_indices.add(extraction_indice)\n",
    "                            cleaned_candidate_extractions.append(extraction)    \n",
    "        \n",
    "                    if len( cleaned_candidate_extractions )>0:\n",
    "                        candidate_indices, candidate_scores = list(zip(*cleaned_candidate_extractions)) \n",
    "                        restored_candidate_indices = []\n",
    "                        for indice in candidate_indices:\n",
    "                            restored_candidate_indices.append([ sub_indices[idx] for idx in indice ] )\n",
    "\n",
    "                        ## update the candidate_scores with rouge_score\n",
    "                        ref_list = [ \"\\n\".join(summary) ] *  len(restored_candidate_indices) \n",
    "                        hyps_list  = []\n",
    "                        for pos in range( len(restored_candidate_indices)  ):\n",
    "                            hyps_list.append( \"\\n\".join( [  data[\"opinion\"][idx] for idx in restored_candidate_indices[pos]  ]  )    )\n",
    "                        candidate_scores = get_real_rouge_score( hyps_list, ref_list, rouge_cal  )\n",
    "\n",
    "                        data[ \"summary_field\" ] = args.summary_field\n",
    "                        data[ \"indices\" ] = restored_candidate_indices\n",
    "                        data[ \"score\" ] = candidate_scores\n",
    "\n",
    "                        fw.write( json.dumps( data  ) + \"\\n\" )\n",
    "                except:\n",
    "                    print(\"Warning! Internal error ...\")    \n",
    "\n",
    "                count +=1\n",
    "\n",
    "    print(\"finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
